{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification on BBC Dataset\n",
    "\n",
    "### Data Description\n",
    "BBC dataset contains 2225 news documents with five different categories. News articles are text files in separate folders.\n",
    "\n",
    "### Concepts Covered\n",
    "- EDA (Exploratory Data Analysis)\n",
    "\n",
    "\n",
    "- Preprocessing\n",
    "    - Tokenization\n",
    "    - Label Encoding\n",
    "    - Stemming & Lemmatization\n",
    "    - Stopwords & Punctuation\n",
    "    - Noise Removal\n",
    "\n",
    "\n",
    "- Vectorization\n",
    "    - Bag of Words\n",
    "    - TF-IDF \n",
    "\n",
    "\n",
    "- Models\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "\n",
    "\n",
    "- Save & Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"U:\\Study\\dataset\\Clustering\\bbc-fulltext\\bbc\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for d in dirs:\n",
    "        class_dir = root + os.sep + d\n",
    "        \n",
    "        for path in os.listdir(class_dir):\n",
    "            try:\n",
    "                with open(class_dir + os.sep + path, 'r') as file:\n",
    "                    text = file.read()\n",
    "                    data.append((text, d))\n",
    "            except:\n",
    "                print(f\"Error while reading data from file: {class_dir + os.sep + path}\")\n",
    "\n",
    "                \n",
    "df = pd.DataFrame(data, columns=['Text', 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the files in text editor then we can see that first line contains the news title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Instances\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category wise count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVO0lEQVR4nO3df9xedX3f8deboFAVhYzAIkHDg6ar0FY68kAtrVJxyuwQZsXGDU2Vjrmh6FZtYe2ctMujWJXqbFnLOkvUKov4g4idBaOAohJCRcIPkUwopKEk4pzaOfoIfvbH+eZ45c5939wkOfd1J/fr+Xhcj+uc73V+fM51znW/73POdc6VqkKSJIADxl2AJGnuMBQkST1DQZLUMxQkST1DQZLUO3DcBeyJww8/vJYuXTruMiRpn3LLLbd8q6oWTfbaPh0KS5cuZcOGDeMuQ5L2KUn+eqrXPHwkSeoZCpKknqEgSeoZCpKknqEgSeoZCpKknqEgSeoZCpKknqEgSert01c076kT3/qBcZewV9zyzteMuwTtQ65//gvGXcJe8YIbrh93CfulQfcUktyXZGOSW5NsaG0Lk1yb5J72fNjI8Bcm2ZTk7iQvGbI2SdKuZuPw0S9W1QlVtbz1XwCsq6plwLrWT5LjgBXA8cBpwKVJFsxCfZKkZhznFM4AVrfu1cCZI+1XVNUjVXUvsAk4aQz1SdK8NXQoFHBNkluSnNvajqyqBwHa8xGt/SjggZFxN7e2nSQ5N8mGJBu2bds2YOmSNP8MfaL55KrakuQI4NokX59m2EzSVrs0VF0GXAawfPnyXV6XJO2+QfcUqmpLe94KfILucNBDSRYDtOetbfDNwNEjoy8BtgxZnyRpZ4OFQpInJzlkRzfwYuB2YC2wsg22Eriqda8FViQ5KMkxwDJg/VD1SZJ2NeThoyOBTyTZMZ8PV9VnktwMrElyDnA/cBZAVd2RZA1wJ7AdOK+qHh2wPknSBIOFQlV9E3j2JO0PA6dOMc4qYNVQNUmSpudtLiRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQ7cNwFSLPt5PedPO4S9pob33jjuEvQfsY9BUlSz1CQJPUMBUlSb/BQSLIgyVeTXN36Fya5Nsk97fmwkWEvTLIpyd1JXjJ0bZKknc3GnsKbgLtG+i8A1lXVMmBd6yfJccAK4HjgNODSJAtmoT5JUjPot4+SLAF+CVgF/PvWfAZwSuteDVwH/GZrv6KqHgHuTbIJOAn48pA1Spo//vDXPzXuEvaaN7z79EGmO/SewnuA3wB+ONJ2ZFU9CNCej2jtRwEPjAy3ubXtJMm5STYk2bBt27ZhqpakeWqwUEjyz4CtVXXLTEeZpK12aai6rKqWV9XyRYsW7VGNkqSdDXn46GTgZUleChwMPDXJh4CHkiyuqgeTLAa2tuE3A0ePjL8E2DJgfZKkCQbbU6iqC6tqSVUtpTuB/LmqOhtYC6xsg60Ermrda4EVSQ5KcgywDFg/VH2SpF2N4zYXFwNrkpwD3A+cBVBVdyRZA9wJbAfOq6pHx1CfJM1bsxIKVXUd3beMqKqHgVOnGG4V3TeVNLD7f+enx13CXvOMt20cdwnSfsMrmiVJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJvcFCIcnBSdYn+VqSO5Jc1NoXJrk2yT3t+bCRcS5MsinJ3UleMlRtkqTJDbmn8Ajwwqp6NnACcFqS5wIXAOuqahmwrvWT5DhgBXA8cBpwaZIFA9YnSZpgsFCozvdb7xPao4AzgNWtfTVwZus+A7iiqh6pqnuBTcBJQ9UnSdrVoOcUkixIciuwFbi2qm4CjqyqBwHa8xFt8KOAB0ZG39zaJk7z3CQbkmzYtm3bkOVL0rwzaChU1aNVdQKwBDgpyU9NM3gmm8Qk07ysqpZX1fJFixbtrVIlScwwFJKsm0nbVKrqO8B1dOcKHkqyuE1jMd1eBHR7BkePjLYE2DLTeUiS9ty0odC+QbQQODzJYe2bQwuTLAWe/hjjLkpyaOv+MeBFwNeBtcDKNthK4KrWvRZYkeSgJMcAy4D1u7dYkqTdceBjvP6vgTfTBcAt/OgQz3eBP3qMcRcDq9s3iA4A1lTV1Um+DKxJcg5wP3AWQFXdkWQNcCewHTivqh7djWWSJO2maUOhqt4LvDfJG6vqfY9nwlV1G/Czk7Q/DJw6xTirgFWPZz6SpL3nsfYUAKiq9yX5OWDp6DhV9YGB6pIkjcGMQiHJB4FjgVuBHYd0CjAUJGk/MqNQAJYDx1XVLl8RlSTtP2Z6ncLtwD8cshBJ0vjNdE/hcODOJOvp7mkEQFW9bJCqJEljMdNQePuQRUiS5oaZfvvo+qELkSSN30y/ffQ9fnQfoifS3fH076rqqUMVJkmafTPdUzhktD/JmXhba0na7+zWXVKr6pPAC/dyLZKkMZvp4aOXj/QeQHfdgtcsSNJ+ZqbfPjp9pHs7cB/dL6VJkvYjMz2n8NqhC5Ekjd9Mf2RnSZJPJNma5KEkH0uyZOjiJEmza6Ynmv+M7kdwnk73u8mfam2SpP3ITENhUVX9WVVtb4/LAX8gWZL2MzMNhW8lOTvJgvY4G3h4yMIkSbNvpqHwOuCVwN8CDwKvADz5LEn7mZl+JfV3gZVV9b8BkiwE3kUXFpKk/cRM9xR+ZkcgAFTVt5nk95clSfu2mYbCAUkO29HT9hRmupchSdpHzPQP+7uBLyW5ku72Fq8EVg1WlSRpLGZ6RfMHkmyguwlegJdX1Z2DViZJmnUzPgTUQsAgkKT92G7dOluStH8yFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJvcFCIcnRST6f5K4kdyR5U2tfmOTaJPe059F7Kl2YZFOSu5O8ZKjaJEmTG3JPYTvw61X1LOC5wHlJjgMuANZV1TJgXeunvbYCOB44Dbg0yYIB65MkTTBYKFTVg1X1V637e8BddL/vfAawug22GjizdZ8BXFFVj1TVvcAm4KSh6pMk7WpWzikkWUr3+ws3AUdW1YPQBQdwRBvsKOCBkdE2t7aJ0zo3yYYkG7Zt2zZk2ZI07wweCkmeAnwMeHNVfXe6QSdpq10aqi6rquVVtXzRokV7q0xJEgOHQpIn0AXCn1fVx1vzQ0kWt9cXA1tb+2bg6JHRlwBbhqxPkrSzIb99FOC/A3dV1SUjL60FVrbulcBVI+0rkhyU5BhgGbB+qPokSbsa8ic1TwZeDWxMcmtr+w/AxcCaJOcA9wNnAVTVHUnW0P1mw3bgvKp6dMD6JEkTDBYKVfVFJj9PAHDqFOOswp/5lKSx8YpmSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVJvsFBI8v4kW5PcPtK2MMm1Se5pz4eNvHZhkk1J7k7ykqHqkiRNbcg9hcuB0ya0XQCsq6plwLrWT5LjgBXA8W2cS5MsGLA2SdIkBguFqroB+PaE5jOA1a17NXDmSPsVVfVIVd0LbAJOGqo2SdLkZvucwpFV9SBAez6itR8FPDAy3ObWtosk5ybZkGTDtm3bBi1WkuabuXKiOZO01WQDVtVlVbW8qpYvWrRo4LIkaX6Z7VB4KMligPa8tbVvBo4eGW4JsGWWa5OkeW+2Q2EtsLJ1rwSuGmlfkeSgJMcAy4D1s1ybJM17Bw414SQfAU4BDk+yGfhPwMXAmiTnAPcDZwFU1R1J1gB3AtuB86rq0aFqkyRNbrBQqKpXTfHSqVMMvwpYNVQ9kqTHNldONEuS5gBDQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUm3OhkOS0JHcn2ZTkgnHXI0nzyZwKhSQLgD8C/ilwHPCqJMeNtypJmj/mVCgAJwGbquqbVfX3wBXAGWOuSZLmjVTVuGvoJXkFcFpV/VrrfzXwnKp6w8gw5wLntt5/BNw964U+PocD3xp3EWMyn5cd5vfyz+dlh7m//M+sqkWTvXDgbFfyGDJJ206pVVWXAZfNTjl7LsmGqlo+7jrGYT4vO8zv5Z/Pyw779vLPtcNHm4GjR/qXAFvGVIskzTtzLRRuBpYlOSbJE4EVwNox1yRJ88acOnxUVduTvAH4S2AB8P6qumPMZe2pfeZQ1wDm87LD/F7++bzssA8v/5w60SxJGq+5dvhIkjRGhoIkqWcojEiyNMnteziNpye5cm/VNLQkZ+7OVeNJTknyczMY7mXjul1JkkOT/NtZmtd1SZa37r9o895p/vvatjG0mW5Dc8WebE9JLm/XYc15hsJeVlVbqmqfWPnNmXS3FJmxJAcCpwCP+YGuqrVVdfHulbbHDgVmJRRGVdVLq+o7E+e/D24bg3k829AcMpbtadZVlY/2AJYCXwdWA7cBVwJPAu4DDm/DLAeua90vAG5tj68Ch7Rp3N5e/1Xg48BngHuA3x+Z14uBLwN/BXwUeEprvxi4s83/Xa3tLOB24GvADTNYjrOB9a2uP6H7Jtf3gVVtGl8BjqT7QH4buLcNe2x7fAa4BfgC8JNtmpcDlwCfBz4G/C3wN228XwBOB25q78NngSNH3oM/HJnGfwG+BHwTeEVrPwW4HlgDfKO9B/+yLcNG4Ng23KI275vb4+TW/nbg/cB1bbrnt/YrgB+0Gt+5l7aFU9sybmzzPKgNfx2wvHXfR3dF607zn7BtLADe1aZzG/DGqdb/XHgATwY+3baf24Ffacv5jrae1gM/3oZ9JrCuLcM64Bkz2YbGvYwzeA8mrs+3tu3wNuCikeFe09q+Bnxwum1/Lj7GXsBcerQPbY38sXk/8BamDoVPjQz7FLqv+I5+8H+1bQBPAw4G/pru4rzDgRuAJ7fhfhN4G7CQ7rYdO74Vdmh73ggcNdo2zTI8q9X1hNZ/adtICzi9tf0+8NsjG+srRsZfByxr3c8BPjcy3NXAgtb/duAtI+MdNlL3rwHvHnkPRkPho3R7qMfR3ecKulD4DrAYOKj9obiovfYm4D2t+8PAz7fuZwB3jdTypTbu4cDDwBNG18Ve2hZ+G3gA+InW9gHgza37OnYNhZ3mP2Hb+Dd0fxgPbP0Lp1r/c+EB/DLw30b6n9aW87da/2uAq0c+Fytb9+uAT85kG5rrjwnr78V0XztN256vBp4PHN/W4Y6/Fwun2/bn4mNOXacwRzxQVTe27g8B508z7I3AJUn+HPh4VW1OdrlTx7qq+j8ASe6k+y/qULoN48Y2/BPp9hq+C/w/4E+TfJpuQ9sxn8uTrKHb85jOqcCJwM1t2j8GbAX+fmR6twD/ZOKISZ5Ct/fw0ZHlOGhkkI9W1aNTzHcJ8D+SLG7Lc+8Uw32yqn4I3JnkyJH2m6vqwVbH/wKuae0bgV9s3S8Cjhup7alJDmndn66qR4BHkmyl2xPaUxO3hf8I3FtV32htq4HzgPfsxrRfBPxxVW0HqKpvt0Mqk63/uWAj8K4k76D74/+Fth4+0l7/CPAHrft5wMtb9wfp/gnZYbptaF/y4vb4aut/CrAMeDZwZVV9C7r1OjLOVNv+nGIo7GrihRsFbOdH518O7l+ourh9eF8KfCXJi+g+1KMeGel+lO49D3BtVb1q4syTnET3h30F8AbghVX1+iTPAX4JuDXJCVX18BT1B1hdVRdOmO5bqv3LMlLHRAcA36mqE6aY9t9N0Q7wPuCSqlqb5BS6/wInM/p+ZIr2H470/3Ck1gOA51XVD0Yn2P44TfY+76khL+LJxOlXd/HmLut/wBpmrKq+keREum3995LsCO3RZZjq/Rptn24b2pcE+L2q+pOdGpPzmfp9mGrbn1M80byrZyR5Xut+FfBFut3kE1vbL+8YMMmxVbWxqt4BbAB+cobz+ApwcpIfb9N5UpKfaP+pP62q/gJ4M3DCyHxuqqq30d158eipJkx3+OcVSY5o4y5M8sxphv8e3bkQquq7wL1JzmrjJsmzH2u85ml0h30AVk4zvz1xDd0fSgCSTBVeO0ys8fGauC18Fli6Y70Br6Y7F7I7878GeH3bO9ixniZd/3NBkqcD/7eqPkR3LuQft5d+ZeT5y637S3ShBt25oS9OMdk9XT+zbbTevwRe19YZSY5qn7l1wCuT/IPWvnAsle4BQ2FXdwErk9xGd4z3vwIXAe9N8gW6/0J3eHOS25N8je4E1P+cyQyqahvdsfaPtPl8hS5QDgGubm3XA/+ujfLOJBvb12VvoDuBNdW076Q79n1Nm861dMfqp3IF8NYkX01yLN2H+Jy2THcw9e9ZfAr450luTfILdHsGH23v0VC3DD4fWJ7ktnYo7vXTDdz2pm5s6+iduzG/idvCHwCvpVvOjXR7MX+8m/P/U+B+4Lb2Xv8Lpl7/c8FPA+uT3Ar8FvCfW/tBSW6iO/ezo97zgde25Xh1e20yE7ehOW10fdIdfv0w8OW2LVwJHFLdbXlWAde39XrJ2AreTd7mQppEkqV0x85/asylzFlJ7qM7uT6XfzdAj5N7CpKknnsKkqSeewqSpJ6hIEnqGQqSpJ6hIM1Aku8/jmHfnuQtQ01fGpKhIEnqGQrSbkpyepKb2oV/n51wP5tnJ/lcknuS/KuRcd6a5OZ2Ad5FYyhbmpahIO2+LwLPraqfpbsy/DdGXvsZuntVPQ94W/uBnRfT3TTtJLpbWJyY5PmzXLM0LW+IJ+2+6e4Me1W7cd8PknyeLgh+nsnvrHnD7JUsTc9QkHbfdHeGnexuu5PeWVOaSzx8JO2+6e4Me0aSg9vdMk+h+4Wuqe6sKc0Z7ilIM/OkJJtH+i/hR3eG/Ru6O90eM/L6erqfr3wG8LtVtQXYkuRZdHfWhO4nUs+m+xEkaU7w3keSpJ6HjyRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJvf8PRWMB06Zwq8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Label', data=df)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "We have almost all class count neary equal, we can say that our dataset is balanced.\n",
    "\n",
    "Further analysis like unique words and most frequent words. We will do it after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization\n",
    "There are multiple ways to tokenize text. Simplest way is to split by whitespace. However, spliting by whitespace doesn't give proper tokens. \n",
    "Here, we will use NLTK library to tokenize text in to sentence, then in to the words. This way, we will get proper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessd_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['Text'].lower()\n",
    "    words =[]\n",
    "    \n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        words.extend(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    preprocessd_data.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stemming & Lemmatization\n",
    "\n",
    "Lemmatization transforms word into the root form. For example, playing, played, plays become play. Stemming also does the same thing almost, but it literally transforms the word based on rules even if word doesn't exists in the language. Lemmatization solves this issue by using the language knowledge. That's why we are using Lemmatization here instead of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_data = []\n",
    "\n",
    "for words in preprocessd_data:\n",
    "    lemmatized_data.append([wordnet_lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Label Encoding\n",
    "\n",
    "Our labels are in string. Machine Learning algorithms need data in numerical format. So, we will transform our labels in to the numbers. For example, sports to 0, business to 1 and so on. To do this, we will use LabelEncoder from sklearn library. \n",
    "\n",
    "fit_transform funcation creates number of integers as per the labels and also converts existing labels into the numbers and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "new_labels = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    511\n",
       "0    510\n",
       "2    417\n",
       "4    401\n",
       "1    386\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analysis\n",
    "\n",
    "Before moving to stopwords and noise removal, let's analyse the unique words and most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter(w for l1 in lemmatized_data for w in set(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31756"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique Words\n",
    "len(word_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2225),\n",
       " ('.', 2225),\n",
       " ('the', 2225),\n",
       " ('a', 2216),\n",
       " ('to', 2214),\n",
       " ('of', 2207),\n",
       " ('and', 2205),\n",
       " ('in', 2204),\n",
       " ('for', 2083),\n",
       " (\"'s\", 2063),\n",
       " ('on', 2015),\n",
       " ('it', 1986),\n",
       " ('is', 1923),\n",
       " ('``', 1922),\n",
       " (\"''\", 1919),\n",
       " ('said', 1888),\n",
       " ('that', 1848),\n",
       " ('with', 1838),\n",
       " ('ha', 1830),\n",
       " ('at', 1769),\n",
       " ('wa', 1754),\n",
       " ('be', 1729),\n",
       " ('by', 1709),\n",
       " ('have', 1668),\n",
       " ('but', 1629),\n",
       " ('from', 1565),\n",
       " ('will', 1470),\n",
       " ('are', 1459),\n",
       " ('an', 1425),\n",
       " ('he', 1406),\n",
       " ('not', 1386),\n",
       " ('which', 1345),\n",
       " ('this', 1343),\n",
       " ('year', 1320),\n",
       " ('been', 1316),\n",
       " ('also', 1265),\n",
       " ('had', 1213),\n",
       " ('would', 1156),\n",
       " ('they', 1152),\n",
       " ('up', 1135),\n",
       " ('their', 1126),\n",
       " ('were', 1121),\n",
       " ('more', 1108),\n",
       " ('-', 1093),\n",
       " ('who', 1092),\n",
       " ('one', 1042),\n",
       " ('we', 1021),\n",
       " ('his', 1013),\n",
       " ('out', 1008),\n",
       " ('after', 1006),\n",
       " ('about', 981),\n",
       " ('new', 978),\n",
       " ('there', 960),\n",
       " ('than', 958),\n",
       " ('over', 922),\n",
       " ('last', 908),\n",
       " ('(', 906),\n",
       " (')', 906),\n",
       " ('all', 896),\n",
       " ('could', 893),\n",
       " ('i', 893),\n",
       " (':', 882),\n",
       " ('time', 880),\n",
       " ('or', 840),\n",
       " ('when', 839),\n",
       " ('u', 835),\n",
       " ('if', 824),\n",
       " ('two', 814),\n",
       " ('can', 805),\n",
       " ('people', 797),\n",
       " ('mr', 795),\n",
       " ('other', 794),\n",
       " ('first', 781),\n",
       " ('into', 778),\n",
       " ('now', 774),\n",
       " (\"'\", 760),\n",
       " ('do', 750),\n",
       " ('some', 718),\n",
       " ('make', 703),\n",
       " ('say', 680),\n",
       " ('world', 666),\n",
       " ('so', 664),\n",
       " ('only', 662),\n",
       " ('no', 650),\n",
       " ('made', 648),\n",
       " ('being', 643),\n",
       " ('what', 638),\n",
       " ('take', 632),\n",
       " ('%', 630),\n",
       " (\"n't\", 628),\n",
       " ('told', 617),\n",
       " ('get', 615),\n",
       " ('them', 604),\n",
       " ('just', 604),\n",
       " ('before', 600),\n",
       " ('against', 596),\n",
       " ('way', 585),\n",
       " ('while', 584),\n",
       " ('back', 580),\n",
       " ('month', 578)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most Frequent Words\n",
    "word_counter.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that there is a lots of noise in the data. Most common words are like and, of, the, etc. So, we need to remove this kind of data because it's not useful. This kind of words are called stopwords and we can use nltk to deal with that. Other characters like ',', '(', '-' are called punctuations and we can use string module for that. Regular expressions are also used to remove noise like URLs, digits, specific type of pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Stopwords & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for words in lemmatized_data:\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in punctuation:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    filtered_data.append(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after filtering: 31606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 2063),\n",
       " ('``', 1922),\n",
       " (\"''\", 1919),\n",
       " ('said', 1888),\n",
       " ('ha', 1830),\n",
       " ('wa', 1754),\n",
       " ('year', 1320),\n",
       " ('also', 1265),\n",
       " ('would', 1156),\n",
       " ('one', 1042),\n",
       " ('new', 978),\n",
       " ('last', 908),\n",
       " ('could', 893),\n",
       " ('time', 880),\n",
       " ('u', 835),\n",
       " ('two', 814),\n",
       " ('people', 797),\n",
       " ('mr', 795),\n",
       " ('first', 781),\n",
       " ('make', 703),\n",
       " ('say', 680),\n",
       " ('world', 666),\n",
       " ('made', 648),\n",
       " ('take', 632),\n",
       " (\"n't\", 628),\n",
       " ('told', 617),\n",
       " ('get', 615),\n",
       " ('way', 585),\n",
       " ('back', 580),\n",
       " ('month', 578),\n",
       " ('added', 577),\n",
       " ('week', 563),\n",
       " ('three', 559),\n",
       " ('number', 547),\n",
       " ('like', 545),\n",
       " ('next', 543),\n",
       " ('company', 535),\n",
       " ('well', 534),\n",
       " ('many', 527),\n",
       " ('uk', 524),\n",
       " ('set', 516),\n",
       " ('go', 500),\n",
       " ('want', 497),\n",
       " ('since', 490),\n",
       " ('come', 488),\n",
       " ('government', 479),\n",
       " ('still', 476),\n",
       " ('bbc', 475),\n",
       " ('day', 471),\n",
       " ('may', 468),\n",
       " ('see', 467),\n",
       " ('firm', 464),\n",
       " ('part', 461),\n",
       " ('good', 460),\n",
       " ('much', 451),\n",
       " ('work', 450),\n",
       " ('game', 444),\n",
       " ('going', 441),\n",
       " ('country', 440),\n",
       " ('second', 424),\n",
       " ('end', 423),\n",
       " ('need', 419),\n",
       " ('however', 417),\n",
       " ('home', 412),\n",
       " ('win', 412),\n",
       " ('right', 406),\n",
       " ('think', 402),\n",
       " ('market', 393),\n",
       " ('group', 393),\n",
       " ('million', 391),\n",
       " ('plan', 383),\n",
       " ('show', 382),\n",
       " ('put', 382),\n",
       " ('expected', 381),\n",
       " ('player', 379),\n",
       " ('five', 378),\n",
       " ('already', 374),\n",
       " ('play', 372),\n",
       " ('news', 370),\n",
       " ('service', 368),\n",
       " ('place', 363),\n",
       " ('best', 359),\n",
       " ('chief', 357),\n",
       " ('minister', 357),\n",
       " ('top', 357),\n",
       " ('used', 355),\n",
       " ('four', 354),\n",
       " ('use', 352),\n",
       " ('even', 352),\n",
       " ('problem', 344),\n",
       " ('hit', 339),\n",
       " ('including', 339),\n",
       " ('former', 335),\n",
       " ('move', 333),\n",
       " ('according', 333),\n",
       " ('help', 331),\n",
       " ('public', 330),\n",
       " ('2004', 328),\n",
       " ('give', 328),\n",
       " ('director', 325)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter_2 = Counter(w for l1 in filtered_data for w in set(l1))\n",
    "\n",
    "# Unique Words\n",
    "print(\"Unique words after filtering:\", len(word_counter_2.keys()))\n",
    "\n",
    "# Most Common Words After Filtering\n",
    "word_counter_2.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that our result is improved but still there are certain words with punctuation and digits like 's, n't, 2004, etc. We can remove it by using regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove word if it contains any other character than alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "\n",
    "noise_clean = re.compile(r\"[^a-z]\")\n",
    "\n",
    "for words in filtered_data:\n",
    "    clean_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if noise_clean.search(word) is None:\n",
    "            clean_words.append(word)\n",
    "    \n",
    "    clean_data.append(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after cleaning noise: 24228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('said', 1888),\n",
       " ('ha', 1830),\n",
       " ('wa', 1754),\n",
       " ('year', 1320),\n",
       " ('also', 1265),\n",
       " ('would', 1156),\n",
       " ('one', 1042),\n",
       " ('new', 978),\n",
       " ('last', 908),\n",
       " ('could', 893),\n",
       " ('time', 880),\n",
       " ('u', 835),\n",
       " ('two', 814),\n",
       " ('people', 797),\n",
       " ('mr', 795),\n",
       " ('first', 781),\n",
       " ('make', 703),\n",
       " ('say', 680),\n",
       " ('world', 666),\n",
       " ('made', 648),\n",
       " ('take', 632),\n",
       " ('told', 617),\n",
       " ('get', 615),\n",
       " ('way', 585),\n",
       " ('back', 580),\n",
       " ('month', 578),\n",
       " ('added', 577),\n",
       " ('week', 563),\n",
       " ('three', 559),\n",
       " ('number', 547),\n",
       " ('like', 545),\n",
       " ('next', 543),\n",
       " ('company', 535),\n",
       " ('well', 534),\n",
       " ('many', 527),\n",
       " ('uk', 524),\n",
       " ('set', 516),\n",
       " ('go', 500),\n",
       " ('want', 497),\n",
       " ('since', 490),\n",
       " ('come', 488),\n",
       " ('government', 479),\n",
       " ('still', 476),\n",
       " ('bbc', 475),\n",
       " ('day', 471),\n",
       " ('may', 468),\n",
       " ('see', 467),\n",
       " ('firm', 464),\n",
       " ('part', 461),\n",
       " ('good', 460),\n",
       " ('much', 451),\n",
       " ('work', 450),\n",
       " ('game', 444),\n",
       " ('going', 441),\n",
       " ('country', 440),\n",
       " ('second', 424),\n",
       " ('end', 423),\n",
       " ('need', 419),\n",
       " ('however', 417),\n",
       " ('home', 412),\n",
       " ('win', 412),\n",
       " ('right', 406),\n",
       " ('think', 402),\n",
       " ('market', 393),\n",
       " ('group', 393),\n",
       " ('million', 391),\n",
       " ('plan', 383),\n",
       " ('show', 382),\n",
       " ('put', 382),\n",
       " ('expected', 381),\n",
       " ('player', 379),\n",
       " ('five', 378),\n",
       " ('already', 374),\n",
       " ('play', 372),\n",
       " ('news', 370),\n",
       " ('service', 368),\n",
       " ('place', 363),\n",
       " ('best', 359),\n",
       " ('chief', 357),\n",
       " ('minister', 357),\n",
       " ('top', 357),\n",
       " ('used', 355),\n",
       " ('four', 354),\n",
       " ('use', 352),\n",
       " ('even', 352),\n",
       " ('problem', 344),\n",
       " ('hit', 339),\n",
       " ('including', 339),\n",
       " ('former', 335),\n",
       " ('move', 333),\n",
       " ('according', 333),\n",
       " ('help', 331),\n",
       " ('public', 330),\n",
       " ('give', 328),\n",
       " ('director', 325),\n",
       " ('sale', 324),\n",
       " ('report', 324),\n",
       " ('six', 321),\n",
       " ('another', 317),\n",
       " ('around', 314)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter_3 = Counter(w for l1 in clean_data for w in set(l1))\n",
    "\n",
    "# Unique Words\n",
    "print(\"Unique words after cleaning noise:\", len(word_counter_3.keys()))\n",
    "\n",
    "# Most Common Words After Filtering\n",
    "word_counter_3.most_common(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
