{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017496,
     "end_time": "2020-12-03T19:38:42.866407",
     "exception": false,
     "start_time": "2020-12-03T19:38:42.848911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:42.918860Z",
     "iopub.status.busy": "2020-12-03T19:38:42.918002Z",
     "iopub.status.idle": "2020-12-03T19:38:50.148094Z",
     "shell.execute_reply": "2020-12-03T19:38:50.146391Z"
    },
    "papermill": {
     "duration": 7.26544,
     "end_time": "2020-12-03T19:38:50.148236",
     "exception": false,
     "start_time": "2020-12-03T19:38:42.882796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import imagesize\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "import sys\n",
    "%matplotlib inline\n",
    "sns.set_style()\n",
    "\n",
    "# to divide our data into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "#to encode our labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#to build our model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout\n",
    "# Our optimizer options\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "#Callback options\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#importing image data generator for data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#for the final prediction report\n",
    "from sklearn.metrics import classification_report ,confusion_matrix\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import save_model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015399,
     "end_time": "2020-12-03T19:38:50.179908",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.164509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.226860Z",
     "iopub.status.busy": "2020-12-03T19:38:50.225876Z",
     "iopub.status.idle": "2020-12-03T19:38:50.360029Z",
     "shell.execute_reply": "2020-12-03T19:38:50.359326Z"
    },
    "papermill": {
     "duration": 0.164618,
     "end_time": "2020-12-03T19:38:50.360155",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.195537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = '../input/til2020/'\n",
    "\n",
    "with open(base_dir + r\"train.json\",\"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "with open(base_dir + r\"val.json\",\"r\") as file:\n",
    "    val_data = json.load(file)\n",
    "    \n",
    "train_annotations = train_data['annotations']\n",
    "train_images = train_data['images']\n",
    "categories = train_data['categories']\n",
    "\n",
    "val_annotations = val_data['annotations']\n",
    "val_images = val_data['images']\n",
    "\n",
    "category_mapping = {}\n",
    "\n",
    "for category_item in categories:\n",
    "    category_mapping[category_item['id']] = category_item['name']\n",
    "\n",
    "train_id_to_path_mapping = {}\n",
    "\n",
    "for image_item in train_images:\n",
    "    train_id_to_path_mapping[image_item['id']] = image_item['file_name']\n",
    "    \n",
    "val_id_to_path_mapping = {}\n",
    "\n",
    "for image_item in val_images:\n",
    "    val_id_to_path_mapping[image_item['id']] = image_item['file_name']\n",
    "    \n",
    "for annotation in train_annotations:\n",
    "    annotation['image_path'] = '../input/til2020/train/train/'+train_id_to_path_mapping[annotation['image_id']]\n",
    "    annotation['cat'] = category_mapping[annotation['category_id']]\n",
    "    annotation['bbox'] = list(map(int,annotation['bbox']))\n",
    "    \n",
    "for annotation in val_annotations:\n",
    "    annotation['image_path'] = '../input/til2020/val/val/'+val_id_to_path_mapping[annotation['image_id']]\n",
    "    annotation['cat'] = category_mapping[annotation['category_id']]\n",
    "    annotation['bbox'] = list(map(int,annotation['bbox']))\n",
    "    \n",
    "annotations = train_annotations + val_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015417,
     "end_time": "2020-12-03T19:38:50.391533",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.376116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Downsample Dresses Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.430400Z",
     "iopub.status.busy": "2020-12-03T19:38:50.429383Z",
     "iopub.status.idle": "2020-12-03T19:38:50.435126Z",
     "shell.execute_reply": "2020-12-03T19:38:50.434491Z"
    },
    "papermill": {
     "duration": 0.028029,
     "end_time": "2020-12-03T19:38:50.435252",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.407223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'name': 'tops'},\n",
       " {'id': 2, 'name': 'trousers'},\n",
       " {'id': 3, 'name': 'outerwear'},\n",
       " {'id': 4, 'name': 'dresses'},\n",
       " {'id': 5, 'name': 'skirts'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.474370Z",
     "iopub.status.busy": "2020-12-03T19:38:50.473433Z",
     "iopub.status.idle": "2020-12-03T19:38:50.477112Z",
     "shell.execute_reply": "2020-12-03T19:38:50.477685Z"
    },
    "papermill": {
     "duration": 0.025988,
     "end_time": "2020-12-03T19:38:50.477812",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.451824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 55769,\n",
       " 'iscrowd': 0,\n",
       " 'id': 2,\n",
       " 'image_id': 1000,\n",
       " 'category_id': 1,\n",
       " 'bbox': [321, 332, 217, 257],\n",
       " 'image_path': '../input/til2020/train/train/1000.jpg',\n",
       " 'cat': 'tops'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.533609Z",
     "iopub.status.busy": "2020-12-03T19:38:50.532528Z",
     "iopub.status.idle": "2020-12-03T19:38:50.542761Z",
     "shell.execute_reply": "2020-12-03T19:38:50.541997Z"
    },
    "papermill": {
     "duration": 0.047636,
     "end_time": "2020-12-03T19:38:50.542916",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.495280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Annotations: 13317\n",
      "Dresses Count: 7585\n",
      "Category Wise Count:\n",
      "{'tops': 945, 'trousers': 1671, 'outerwear': 1486, 'dresses': 7585, 'skirts': 1630}\n",
      "New Train Annotations: 13317\n"
     ]
    }
   ],
   "source": [
    "indexes = []\n",
    "train_annotations_2 = []\n",
    "\n",
    "counts = {'tops':0,'trousers':0,'outerwear':0,'dresses':0,'skirts':0,}\n",
    "\n",
    "for idx, annot in enumerate(train_annotations):\n",
    "    if annot['cat'] == 'dresses':\n",
    "        indexes.append(idx)\n",
    "    else:\n",
    "        train_annotations_2.append(annot)\n",
    "        \n",
    "    counts[annot['cat']]+=1\n",
    "\n",
    "print(\"Total Train Annotations:\", len(train_annotations))\n",
    "print(\"Dresses Count:\", len(indexes))\n",
    "\n",
    "print(\"Category Wise Count:\")\n",
    "print(counts)\n",
    "\n",
    "max_samples = 8500\n",
    "\n",
    "for i in indexes[:max_samples]:\n",
    "    train_annotations_2.append(train_annotations[i])\n",
    "\n",
    "print(\"New Train Annotations:\", len(train_annotations_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017735,
     "end_time": "2020-12-03T19:38:50.578278",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.560543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Confirm Number of Noise Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.618261Z",
     "iopub.status.busy": "2020-12-03T19:38:50.617418Z",
     "iopub.status.idle": "2020-12-03T19:38:50.682751Z",
     "shell.execute_reply": "2020-12-03T19:38:50.681951Z"
    },
    "papermill": {
     "duration": 0.086897,
     "end_time": "2020-12-03T19:38:50.682908",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.596011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3704\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(len(os.listdir('../input/noise-image-generation')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017868,
     "end_time": "2020-12-03T19:38:50.719246",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.701378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Images To Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.768064Z",
     "iopub.status.busy": "2020-12-03T19:38:50.766991Z",
     "iopub.status.idle": "2020-12-03T19:38:50.786644Z",
     "shell.execute_reply": "2020-12-03T19:38:50.787557Z"
    },
    "papermill": {
     "duration": 0.050518,
     "end_time": "2020-12-03T19:38:50.787770",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.737252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'name': 'tops'}, {'id': 2, 'name': 'trousers'}, {'id': 3, 'name': 'outerwear'}, {'id': 4, 'name': 'dresses'}, {'id': 5, 'name': 'skirts'}, {'id': 6, 'name': 'noise'}]\n",
      "['tops', 'trousers', 'outerwear', 'dresses', 'skirts', 'noise']\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "{'tops': array([1., 0., 0., 0., 0., 0.], dtype=float32), 'trousers': array([0., 1., 0., 0., 0., 0.], dtype=float32), 'outerwear': array([0., 0., 1., 0., 0., 0.], dtype=float32), 'dresses': array([0., 0., 0., 1., 0., 0.], dtype=float32), 'skirts': array([0., 0., 0., 0., 1., 0.], dtype=float32), 'noise': array([0., 0., 0., 0., 0., 1.], dtype=float32)}\n",
      "16517\n"
     ]
    }
   ],
   "source": [
    "# Adding Noise Category Id and Name\n",
    "categories.append({'id':6, 'name':'noise'})\n",
    "print(categories)\n",
    "\n",
    "# Converting Categories to One-Hot Encoded Vectors \n",
    "new_categories = [x['name'] for x in categories]\n",
    "print(new_categories)\n",
    "encoded_categories = to_categorical(list(range(len(new_categories))), num_classes=len(new_categories))\n",
    "print(encoded_categories)\n",
    "\n",
    "category_mapping = {x:encoded_categories[i] for i,x in enumerate(new_categories)}\n",
    "print(category_mapping)\n",
    "\n",
    "# Add Noise Images to Annotations\n",
    "for i, path in enumerate(os.listdir('../input/noise-image-generation')):\n",
    "    record = {'area': None,\n",
    "             'iscrowd': 0,\n",
    "             'id': -1,\n",
    "             'image_id': -1,\n",
    "             'category_id': 6,\n",
    "             'bbox': None,\n",
    "             'image_path': '../input/image-classification/'+path,\n",
    "             'cat': 'noise'}\n",
    "    \n",
    "    if i < 3200:\n",
    "        train_annotations_2.append(record)\n",
    "    else:\n",
    "        val_annotations.append(record)\n",
    "    \n",
    "print(len(train_annotations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.837323Z",
     "iopub.status.busy": "2020-12-03T19:38:50.836277Z",
     "iopub.status.idle": "2020-12-03T19:38:50.839828Z",
     "shell.execute_reply": "2020-12-03T19:38:50.839200Z"
    },
    "papermill": {
     "duration": 0.033171,
     "end_time": "2020-12-03T19:38:50.839944",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.806773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cropped_image(img, bbox):\n",
    "    start_x, start_y, width, height = bbox\n",
    "    cropped_img = img[start_y:start_y+height, start_x:start_x+width]\n",
    "    return cropped_img\n",
    "      \n",
    "def get_reshaped_image(img, new_shape=(224,224)):\n",
    "    resized_image = cv2.resize(img, new_shape, interpolation = cv2.INTER_NEAREST) \n",
    "    return resized_image\n",
    "\n",
    "def rescale_bbox(bbox, current_img_shape, new_img_shape=(224,224)):\n",
    "    x_ratio = new_img_shape[0] / current_img_shape[0]\n",
    "    y_ratio = new_img_shape[1] / current_img_shape[1]\n",
    "    \n",
    "    new_x = bbox[0] * x_ratio\n",
    "    new_y = bbox[1] * y_ratio\n",
    "    new_width = bbox[2] * x_ratio\n",
    "    new_height = bbox[3] * y_ratio\n",
    "    \n",
    "    return new_x, new_y, new_width, new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:38:50.903084Z",
     "iopub.status.busy": "2020-12-03T19:38:50.902217Z",
     "iopub.status.idle": "2020-12-03T19:57:29.897528Z",
     "shell.execute_reply": "2020-12-03T19:57:29.898124Z"
    },
    "papermill": {
     "duration": 1119.03925,
     "end_time": "2020-12-03T19:57:29.898307",
     "exception": false,
     "start_time": "2020-12-03T19:38:50.859057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Images:  1000\n",
      "Processed Images:  2000\n",
      "Processed Images:  3000\n",
      "Processed Images:  4000\n",
      "Processed Images:  5000\n",
      "Processed Images:  6000\n",
      "Processed Images:  7000\n",
      "Processed Images:  8000\n",
      "Processed Images:  9000\n",
      "Processed Images:  10000\n",
      "Processed Images:  11000\n",
      "Processed Images:  12000\n",
      "Processed Images:  13000\n",
      "19049\n",
      "19049\n",
      "Processed Images:  1000\n",
      "Processed Images:  2000\n",
      "19049\n",
      "19049\n",
      "(19049, 128, 128, 3)\n",
      "(19049, 6)\n",
      "3578\n",
      "3578\n",
      "(3578, 128, 128, 3)\n",
      "(3578, 6)\n"
     ]
    }
   ],
   "source": [
    "ignore_flip = ('dresses', 'noise')\n",
    "\n",
    "def transform_data(annotations, samples_per_cat=None, cats=None):\n",
    "    features = []\n",
    "    labels = []\n",
    "    max_check = False\n",
    "    cat_count = {}\n",
    "    \n",
    "    if samples_per_cat is not None:\n",
    "        max_check = True\n",
    "        cat_count = {x:0 for x in cats}\n",
    "    else:\n",
    "        samples_per_cat = sys.maxsize\n",
    "        \n",
    "    \n",
    "    for i, annotation in enumerate(annotations):\n",
    "        img_path = annotation['image_path']\n",
    "        cat = annotation['cat']\n",
    "        bbox = annotation['bbox']\n",
    "\n",
    "        try:\n",
    "            if max_check:\n",
    "                if cat in cats:\n",
    "                    if cat_count[cat] >= samples_per_cat:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            if cat == 'noise':\n",
    "                resized_image = get_reshaped_image(img, new_shape=(128,128))\n",
    "            else:\n",
    "                #x,y,w,h = rescale_bbox(bbox, (img.shape[0],img.shape[1]))\n",
    "                cropped_image = get_cropped_image(img, bbox)\n",
    "                resized_image = get_reshaped_image(cropped_image, new_shape=(128,128))\n",
    "\n",
    "            features.append(resized_image)\n",
    "            labels.append(category_mapping[cat])\n",
    "\n",
    "            cat_count[cat] += 1\n",
    "            \n",
    "            if cat not in ignore_flip:\n",
    "                features.append(cv2.flip(resized_image,1))\n",
    "                labels.append(category_mapping[cat])\n",
    "                cat_count[cat] += 1\n",
    "            \n",
    "            if i != 0 and i % 1000 == 0:\n",
    "                print(\"Processed Images: \",i)\n",
    "\n",
    "            #print(resized_image.shape)\n",
    "\n",
    "            #plt.imshow(resized_image)\n",
    "            #plt.title(cat)\n",
    "            #plt.show()\n",
    "        except:\n",
    "            print(f\"Error in image: bbox={bbox}, img_path={img_path}, cat={cat}\")\n",
    "            traceback.print_exc()\n",
    "        \n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "max_samples = 10000\n",
    "# cats = {'tops','trousers'}\n",
    "cats = set(new_categories)\n",
    "    \n",
    "train_features, train_labels = transform_data(train_annotations_2, samples_per_cat=max_samples, cats=cats)\n",
    "    \n",
    "print(len(train_features))\n",
    "print(len(train_labels))\n",
    "\n",
    "#print(train_data[0])\n",
    "#print(labels[0])\n",
    "\n",
    "test_features, test_labels = transform_data(val_annotations, samples_per_cat=max_samples, cats=cats)\n",
    "\n",
    "print(len(train_features))\n",
    "print(len(train_labels))\n",
    "\n",
    "train_features_2 = np.asarray(train_features)\n",
    "print(train_features_2.shape)\n",
    "train_labels_2 = np.asarray(train_labels)\n",
    "print(train_labels_2.shape)\n",
    "\n",
    "print(len(test_features))\n",
    "print(len(test_labels))\n",
    "\n",
    "test_features_2 = np.asarray(test_features)\n",
    "print(test_features_2.shape)\n",
    "test_labels_2 = np.asarray(test_labels)\n",
    "print(test_labels_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025811,
     "end_time": "2020-12-03T19:57:29.950452",
     "exception": false,
     "start_time": "2020-12-03T19:57:29.924641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:57:30.008630Z",
     "iopub.status.busy": "2020-12-03T19:57:30.007874Z",
     "iopub.status.idle": "2020-12-03T19:57:30.012874Z",
     "shell.execute_reply": "2020-12-03T19:57:30.012256Z"
    },
    "papermill": {
     "duration": 0.036976,
     "end_time": "2020-12-03T19:57:30.012997",
     "exception": false,
     "start_time": "2020-12-03T19:57:29.976021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_shape = (128, 128, 3)\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:57:30.070828Z",
     "iopub.status.busy": "2020-12-03T19:57:30.070044Z",
     "iopub.status.idle": "2020-12-03T19:57:30.074171Z",
     "shell.execute_reply": "2020-12-03T19:57:30.074698Z"
    },
    "papermill": {
     "duration": 0.035891,
     "end_time": "2020-12-03T19:57:30.074859",
     "exception": false,
     "start_time": "2020-12-03T19:57:30.038968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "# input_shape = (128, 128, 3)\n",
    "# epoch = 100\n",
    "\n",
    "# resnet_50 = ResNet50(weights=None, input_shape=input_shape, classes=len(categories))\n",
    "\n",
    "# summarize the model\n",
    "# resnet_50.summary()\n",
    "\n",
    "# resnet_50.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# early_stop= EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "#                                             patience=10, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)\n",
    "\n",
    "# mcp_save = ModelCheckpoint('.mdl_resnet50_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# resnet_50.fit(train_features_2, train_labels_2,\n",
    "#           epochs=epoch,\n",
    "#           batch_size=batch_size,\n",
    "#           validation_data=(test_features_2,test_labels_2), \n",
    "#           callbacks=[early_stop, mcp_save, learning_rate_reduction])\n",
    "\n",
    "# metrics=pd.DataFrame(resnet_50.history.history)\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026251,
     "end_time": "2020-12-03T19:57:30.127439",
     "exception": false,
     "start_time": "2020-12-03T19:57:30.101188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:57:30.185861Z",
     "iopub.status.busy": "2020-12-03T19:57:30.184953Z",
     "iopub.status.idle": "2020-12-03T19:57:30.188633Z",
     "shell.execute_reply": "2020-12-03T19:57:30.189161Z"
    },
    "papermill": {
     "duration": 0.035802,
     "end_time": "2020-12-03T19:57:30.189328",
     "exception": false,
     "start_time": "2020-12-03T19:57:30.153526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inception_v3 = InceptionV3(weights=None, input_shape=input_shape, classes=len(categories))\n",
    "\n",
    "# summarize the model\n",
    "# inception_v3.summary()\n",
    "\n",
    "# inception_v3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# early_stop= EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "#                                             patience=10, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)\n",
    "\n",
    "# mcp_save = ModelCheckpoint('.mdl_inceptionv3_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# inception_v3.fit(train_features_2, train_labels_2,\n",
    "#           epochs=epoch,\n",
    "#           batch_size=batch_size,\n",
    "#           validation_data=(test_features_2,test_labels_2), \n",
    "#           callbacks=[early_stop, mcp_save, learning_rate_reduction])\n",
    "\n",
    "# metrics=pd.DataFrame(inception_v3.history.history)\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026117,
     "end_time": "2020-12-03T19:57:30.242508",
     "exception": false,
     "start_time": "2020-12-03T19:57:30.216391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T19:57:30.309842Z",
     "iopub.status.busy": "2020-12-03T19:57:30.309073Z",
     "iopub.status.idle": "2020-12-03T20:08:37.073355Z",
     "shell.execute_reply": "2020-12-03T20:08:37.072196Z"
    },
    "papermill": {
     "duration": 666.80427,
     "end_time": "2020-12-03T20:08:37.073490",
     "exception": false,
     "start_time": "2020-12-03T19:57:30.269220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "298/298 [==============================] - 45s 152ms/step - loss: 20.7887 - accuracy: 0.3852 - val_loss: 1.5642 - val_accuracy: 0.3740\n",
      "Epoch 2/100\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.5208 - accuracy: 0.3969 - val_loss: 1.5677 - val_accuracy: 0.3740\n",
      "Epoch 3/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4974 - accuracy: 0.3984 - val_loss: 1.5582 - val_accuracy: 0.3740\n",
      "Epoch 4/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4952 - accuracy: 0.3982 - val_loss: 1.5663 - val_accuracy: 0.3740\n",
      "Epoch 5/100\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.4948 - accuracy: 0.3982 - val_loss: 1.5500 - val_accuracy: 0.3740\n",
      "Epoch 6/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4948 - accuracy: 0.3982 - val_loss: 1.5611 - val_accuracy: 0.3740\n",
      "Epoch 7/100\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.4947 - accuracy: 0.3982 - val_loss: 1.5533 - val_accuracy: 0.3740\n",
      "Epoch 8/100\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.4945 - accuracy: 0.3982 - val_loss: 1.5642 - val_accuracy: 0.3740\n",
      "Epoch 9/100\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.4946 - accuracy: 0.3982 - val_loss: 1.5546 - val_accuracy: 0.3740\n",
      "Epoch 10/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4943 - accuracy: 0.3982 - val_loss: 1.5604 - val_accuracy: 0.3740\n",
      "Epoch 11/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 1.4941 - accuracy: 0.3982\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "298/298 [==============================] - 43s 144ms/step - loss: 1.4941 - accuracy: 0.3982 - val_loss: 1.5650 - val_accuracy: 0.3740\n",
      "Epoch 12/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4937 - accuracy: 0.3982 - val_loss: 1.5566 - val_accuracy: 0.3740\n",
      "Epoch 13/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4938 - accuracy: 0.3982 - val_loss: 1.5542 - val_accuracy: 0.3740\n",
      "Epoch 14/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4937 - accuracy: 0.3982 - val_loss: 1.5532 - val_accuracy: 0.3740\n",
      "Epoch 15/100\n",
      "298/298 [==============================] - 43s 145ms/step - loss: 1.4936 - accuracy: 0.3982 - val_loss: 1.5670 - val_accuracy: 0.3740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.788681</td>\n",
       "      <td>0.385165</td>\n",
       "      <td>1.564201</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.520821</td>\n",
       "      <td>0.396924</td>\n",
       "      <td>1.567742</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.497449</td>\n",
       "      <td>0.398394</td>\n",
       "      <td>1.558220</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.495225</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.566278</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.494835</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.549967</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.494762</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.561114</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.494678</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.553253</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.494480</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.564214</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.494635</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.554610</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.494267</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.560419</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.494069</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.565002</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.493690</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.556553</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.493781</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.554248</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.493728</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.553174</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.493565</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>1.566985</td>\n",
       "      <td>0.373952</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy      lr\n",
       "0   20.788681  0.385165  1.564201      0.373952  0.0010\n",
       "1    1.520821  0.396924  1.567742      0.373952  0.0010\n",
       "2    1.497449  0.398394  1.558220      0.373952  0.0010\n",
       "3    1.495225  0.398184  1.566278      0.373952  0.0010\n",
       "4    1.494835  0.398184  1.549967      0.373952  0.0010\n",
       "5    1.494762  0.398184  1.561114      0.373952  0.0010\n",
       "6    1.494678  0.398184  1.553253      0.373952  0.0010\n",
       "7    1.494480  0.398184  1.564214      0.373952  0.0010\n",
       "8    1.494635  0.398184  1.554610      0.373952  0.0010\n",
       "9    1.494267  0.398184  1.560419      0.373952  0.0010\n",
       "10   1.494069  0.398184  1.565002      0.373952  0.0010\n",
       "11   1.493690  0.398184  1.556553      0.373952  0.0005\n",
       "12   1.493781  0.398184  1.554248      0.373952  0.0005\n",
       "13   1.493728  0.398184  1.553174      0.373952  0.0005\n",
       "14   1.493565  0.398184  1.566985      0.373952  0.0005"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = VGG16(weights=None, input_shape=input_shape, classes=len(categories))\n",
    "\n",
    "# summarize the model\n",
    "# vgg16.summary()\n",
    "\n",
    "vgg16.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "early_stop= EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "mcp_save = ModelCheckpoint('mdl_vgg16_wts_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=10, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "vgg16.fit(train_features_2, train_labels_2,\n",
    "          epochs=epoch,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(test_features_2,test_labels_2), \n",
    "          callbacks=[early_stop, learning_rate_reduction])\n",
    "\n",
    "# Save the model\n",
    "filepath = './vgg16_2'\n",
    "save_model(vgg16, filepath)\n",
    "\n",
    "metrics=pd.DataFrame(vgg16.history.history)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1803.077271,
   "end_time": "2020-12-03T20:08:40.762552",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-03T19:38:37.685281",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
