{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Average Width & Height\n",
    "In this notebook, we will calculate avergae dimensions of each category. After that, we will crop images based on that and see whether are we able to get precise sub-images or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "base_dir = r\"U:\\Study\\Durham_AI_Course\\CapstoneOne\\data\\\\\"\n",
    "\n",
    "with open(base_dir + r\"train.json\",\"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "with open(base_dir + r\"val.json\",\"r\") as file:\n",
    "    val_data = json.load(file)\n",
    "    \n",
    "train_annotations = train_data['annotations']\n",
    "train_images = train_data['images']\n",
    "categories = train_data['categories']\n",
    "\n",
    "val_annotations = val_data['annotations']\n",
    "val_images = val_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'area': 2304645, 'iscrowd': 0, 'id': 1, 'image_id': 10, 'category_id': 4, 'bbox': [704, 620, 1401, 1645], 'image_path': 'train\\\\train\\\\10.jpg', 'cat': 'dresses'}, {'area': 55769, 'iscrowd': 0, 'id': 2, 'image_id': 1000, 'category_id': 1, 'bbox': [321, 332, 217, 257], 'image_path': 'train\\\\train\\\\1000.jpg', 'cat': 'tops'}, {'area': 17108, 'iscrowd': 0, 'id': 3, 'image_id': 10003, 'category_id': 2, 'bbox': [220, 758, 188, 91], 'image_path': 'train\\\\train\\\\10003.jpg', 'cat': 'trousers'}, {'area': 117909, 'iscrowd': 0, 'id': 4, 'image_id': 10003, 'category_id': 5, 'bbox': [150, 397, 297, 397], 'image_path': 'train\\\\train\\\\10003.jpg', 'cat': 'skirts'}, {'area': 105925, 'iscrowd': 0, 'id': 5, 'image_id': 10003, 'category_id': 1, 'bbox': [85, 207, 475, 223], 'image_path': 'train\\\\train\\\\10003.jpg', 'cat': 'tops'}, {'area': 3314505, 'iscrowd': 0, 'id': 6, 'image_id': 10005, 'category_id': 4, 'bbox': [1971, 2059, 1483, 2235], 'image_path': 'train\\\\train\\\\10005.jpg', 'cat': 'dresses'}, {'area'\n",
      "[{'area': 996743, 'iscrowd': 0, 'id': 1, 'image_id': 1, 'category_id': 4, 'bbox': [523, 55, 869, 1147], 'image_path': 'val\\\\val\\\\1.jpg', 'cat': 'dresses'}, {'area': 391795, 'iscrowd': 0, 'id': 2, 'image_id': 10000, 'category_id': 2, 'bbox': [182, 2163, 635, 617], 'image_path': 'val\\\\val\\\\10000.jpg', 'cat': 'trousers'}, {'area': 2443350, 'iscrowd': 0, 'id': 3, 'image_id': 10000, 'category_id': 4, 'bbox': [70, 16, 1074, 2275], 'image_path': 'val\\\\val\\\\10000.jpg', 'cat': 'dresses'}, {'area': 294168, 'iscrowd': 0, 'id': 4, 'image_id': 10004, 'category_id': 4, 'bbox': [552, 564, 357, 824], 'image_path': 'val\\\\val\\\\10004.jpg', 'cat': 'dresses'}, {'area': 748984, 'iscrowd': 0, 'id': 5, 'image_id': 10006, 'category_id': 4, 'bbox': [58, 289, 1004, 746], 'image_path': 'val\\\\val\\\\10006.jpg', 'cat': 'dresses'}, {'area': 2030553, 'iscrowd': 0, 'id': 6, 'image_id': 10011, 'category_id': 5, 'bbox': [740, 1971, 1351, 1503], 'image_path': 'val\\\\val\\\\10011.jpg', 'cat': 'skirts'}, {'area': 1627469, 'iscr\n"
     ]
    }
   ],
   "source": [
    "category_mapping = {}\n",
    "\n",
    "for category_item in categories:\n",
    "    category_mapping[category_item['id']] = category_item['name']\n",
    "\n",
    "train_id_to_path_mapping = {}\n",
    "\n",
    "for image_item in train_images:\n",
    "    train_id_to_path_mapping[image_item['id']] = image_item['file_name']\n",
    "    \n",
    "val_id_to_path_mapping = {}\n",
    "\n",
    "for image_item in val_images:\n",
    "    val_id_to_path_mapping[image_item['id']] = image_item['file_name']\n",
    "    \n",
    "for annotation in train_annotations:\n",
    "    annotation['image_path'] = 'train\\\\train\\\\'+train_id_to_path_mapping[annotation['image_id']]\n",
    "    annotation['cat'] = category_mapping[annotation['category_id']]\n",
    "    annotation['bbox'] = list(map(int,annotation['bbox']))\n",
    "    \n",
    "for annotation in val_annotations:\n",
    "    annotation['image_path'] = 'val\\\\val\\\\'+val_id_to_path_mapping[annotation['image_id']]\n",
    "    annotation['cat'] = category_mapping[annotation['category_id']]\n",
    "    annotation['bbox'] = list(map(int,annotation['bbox']))\n",
    "    \n",
    "print(\"%.1000s\" % train_annotations)\n",
    "print(\"%.1000s\" % val_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13317"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Annotations\n",
    "annotations = train_annotations + val_annotations\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'tops', 2: 'trousers', 3: 'outerwear', 4: 'dresses', 5: 'skirts'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect category wise dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility funcations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_image(img, bbox):\n",
    "    start_x, start_y, width, height = bbox\n",
    "    cropped_img = img[start_y:start_y+height, start_x:start_x+width]\n",
    "    return cropped_img\n",
    "      \n",
    "def get_reshaped_image(img, new_shape=(224,224)):\n",
    "    resized_image = cv2.resize(img, new_shape, interpolation = cv2.INTER_NEAREST) \n",
    "    return resized_image\n",
    "\n",
    "def rescale_bbox(bbox, current_img_shape, new_img_shape=(224,224)):\n",
    "    x_ratio = new_img_shape[0] / current_img_shape[0]\n",
    "    y_ratio = new_img_shape[1] / current_img_shape[1]\n",
    "    \n",
    "    new_x = bbox[0] * x_ratio\n",
    "    new_y = bbox[1] * y_ratio\n",
    "    new_width = bbox[2] * x_ratio\n",
    "    new_height = bbox[3] * y_ratio\n",
    "    \n",
    "    return new_x, new_y, new_width, new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15775/15775 [57:09<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: 1, Width Avg: 32.077987395031144, Height Avg: 46.673156901322756\n",
      "Category: 2, Width Avg: 22.002879686807113, Height Avg: 44.81315624124854\n",
      "Category: 3, Width Avg: 41.15369807284264, Height Avg: 58.66411483191084\n",
      "Category: 4, Width Avg: 40.08467534174697, Height Avg: 81.91231424485893\n",
      "Category: 5, Width Avg: 39.87580947143186, Height Avg: 55.75588302093967\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "new_image_shape = (128,128)\n",
    "\n",
    "dimensiosn = {\n",
    "    1: {'width':[], 'height':[]},\n",
    "    2: {'width':[], 'height':[]},\n",
    "    3: {'width':[], 'height':[]},\n",
    "    4: {'width':[], 'height':[]},\n",
    "    5: {'width':[], 'height':[]},\n",
    "}\n",
    "\n",
    "# bbox -> x, y, width, height\n",
    "for annot in tqdm(annotations):\n",
    "    cat = annot['category_id']\n",
    "    bbox = annot['bbox']\n",
    "    \n",
    "    img = cv2.imread(base_dir+annot['image_path'])\n",
    "    bbox = rescale_bbox(bbox,img.shape,new_img_shape=new_image_shape)\n",
    "    \n",
    "    dimensiosn[cat]['width'].append(bbox[2])\n",
    "    dimensiosn[cat]['height'].append(bbox[3])\n",
    "    \n",
    "for cat, items in dimensiosn.items():\n",
    "    total = len(items['width'])\n",
    "    width_avg = sum(items['width'])/total\n",
    "    height_avg = sum(items['height'])/total\n",
    "    \n",
    "    print(f\"Category: {cat}, Width Avg: {width_avg}, Height Avg: {height_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
